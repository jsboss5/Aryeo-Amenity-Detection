{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\r\n",
      "This is free software; see the source for copying conditions.  There is NO\r\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "torch.__version__\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"train\"\n",
    "valid_path = \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classes of amenities Airbnb mostly cares about\n",
    "subset = [\"Toilet\",\n",
    "          \"Swimming_pool\",\n",
    "          \"Bed\",\n",
    "          \"Billiard_table\",\n",
    "          \"Sink\",\n",
    "          \"Fountain\",\n",
    "          \"Oven\",\n",
    "          \"Ceiling_fan\",\n",
    "          \"Television\",\n",
    "          \"Microwave_oven\",\n",
    "          \"Gas_stove\",\n",
    "          \"Refrigerator\",\n",
    "          \"Kitchen_&_dining_room_table\",\n",
    "          \"Washing_machine\",\n",
    "          \"Bathtub\",\n",
    "          \"Stairs\",\n",
    "          \"Fireplace\",\n",
    "          \"Pillow\",\n",
    "          \"Mirror\",\n",
    "          \"Shower\",\n",
    "          \"Couch\",\n",
    "          \"Countertop\",\n",
    "          \"Coffeemaker\",\n",
    "          \"Dishwasher\",\n",
    "          \"Sofa_bed\",\n",
    "          \"Tree_house\",\n",
    "          \"Towel\",\n",
    "          \"Porch\",\n",
    "          \"Wine_rack\",\n",
    "          \"Jacuzzi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Toilet', 'Swimming pool', 'Bed', 'Billiard table', 'Sink', 'Fountain', 'Oven', 'Ceiling fan', 'Television', 'Microwave oven', 'Gas stove', 'Refrigerator', 'Kitchen & dining room table', 'Washing machine', 'Bathtub', 'Stairs', 'Fireplace', 'Pillow', 'Mirror', 'Shower', 'Couch', 'Countertop', 'Coffeemaker', 'Dishwasher', 'Sofa bed', 'Tree house', 'Towel', 'Porch', 'Wine rack', 'Jacuzzi']\n"
     ]
    }
   ],
   "source": [
    "#Replaces underscores with spaces - matches airbnb classes with those from website\n",
    "for i in range(len(subset)):\n",
    "  subset[i] = subset[i].replace(\"_\", \" \")\n",
    "\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toilet,Swimming pool,Bed,Billiard table,Sink,Fountain,Oven,Ceiling fan,Television,Microwave oven,Gas stove,Refrigerator,Kitchen & dining room table,Washing machine,Bathtub,Stairs,Fireplace,Pillow,Mirror,Shower,Couch,Countertop,Coffeemaker,Dishwasher,Sofa bed,Tree house,Towel,Porch,Wine rack,Jacuzzi\n"
     ]
    }
   ],
   "source": [
    "#convert this list to a string for the command line\n",
    "subset_string = str()\n",
    "for amenity in subset:\n",
    "  subset_string += str(amenity) + \",\"\n",
    "subset_string = subset_string[:len(subset_string)-1]\n",
    "print(subset_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns a list of image IDs\n",
    "def get_image_ids(image_folder=None):\n",
    "    \"\"\"\n",
    "    Explores a folder of images and gets their ID from their file name.\n",
    "    Returns a list of all image ID's in image_folder.\n",
    "    E.g. image_folder/608fda8c976e0ac.jpg -> [\"608fda8c976e0ac\"]\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    image_folder (str): path to folder of images, e.g. \"../validation/\"\n",
    "    \"\"\"\n",
    "    return [os.path.splitext(img_name)[0] for img_name in os.listdir(image_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a function which formats a specific annotations csv based on what we're dealing with\n",
    "def format_annotations(image_folder, annotation_file, target_classes=None):\n",
    "    \"\"\"\n",
    "    TODO - NOTE: This function could (definitely can) be faster.\n",
    "    TODO - Some ideas: skip the use of pandas entirely and use CSV's\n",
    "    \n",
    "    Formats annotation_file based on images contained in image_folder.\n",
    "    Will get all unique image IDs and make sure annotation_file\n",
    "    only contains those (the target images).\n",
    "    Adds meta-data to annotation_file such as class names and categories.\n",
    "    If target_classes isn't None, the returned annotations will be filtered by this list.\n",
    "    Note: image_folder and annotation_file should both be validation if working on\n",
    "    validation set or both be training if working on training set.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    image_folder (str): path to folder of target images.\n",
    "    annotation_file (str): path to annotation file of target images.\n",
    "    target_classes (list), optional: a list of target classes you'd like to filter labels.\n",
    "    \"\"\"\n",
    "    # Get all image ids from target directory\n",
    "    image_ids = get_image_ids(image_folder)\n",
    "    \n",
    "    # Setup annotation file and classnames\n",
    "    annot_file = pd.read_csv(annotation_file)\n",
    "    classes = pd.read_csv(\"class-descriptions-boxable.csv\",\n",
    "                          names=[\"LabelName\", \"ClassName\"])\n",
    "    \n",
    "    # Create classname column on annotations which converts label codes to string labels\n",
    "    annot_file[\"ClassName\"] = annot_file[\"LabelName\"].map(classes.set_index(\"LabelName\")[\"ClassName\"])\n",
    "    \n",
    "    # Make sure we only get the images we're concerned about\n",
    "    if target_classes:\n",
    "        annot_file = annot_file[annot_file[\"ImageID\"].isin(image_ids) & annot_file[\"ClassName\"].isin(target_classes)]\n",
    "    else:\n",
    "        annot_file = annot_file[annot_file[\"ImageID\"].isin(image_ids)]\n",
    "   \n",
    "    # Add ClassID column, e.g. \"Bathtub, Toilet\" -> 1, 2\n",
    "    annot_file[\"ClassID\"] = pd.Categorical(annot_file[\"ClassName\"]).codes\n",
    "    \n",
    "    return annot_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_to_absolute(bbox, height, width):\n",
    "    \"\"\"\n",
    "    Converts bounding box dimensions from relative to absolute pixel values (Detectron2 style).\n",
    "    See: https://detectron2.readthedocs.io/modules/structures.html#detectron2.structures.BoxMode\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    bbox (array): relative dimensions of bounding box in format (x0, y0, x1, y1 or Xmin, Ymin, Xmax, Ymax)\n",
    "    height (int): height of image\n",
    "    width (int): width of image\n",
    "    \"\"\"\n",
    "    bbox[0] = np.multiply(bbox[0], width) # x0\n",
    "    bbox[1] = np.multiply(bbox[1], height) # y0\n",
    "    bbox[2] = np.multiply(bbox[2], width) # x1\n",
    "    bbox[3] = np.multiply(bbox[3], height) # y1\n",
    "    #return list(np.round(bbox).astype(float))\n",
    "    return list(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import CV2 for getting height & width of image\n",
    "import cv2\n",
    "\n",
    "# Import Detectron2 BoxMode for bounding boxes style\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_classes = [\"Coffeemaker\",\"Dishwasher\",\"Sofa bed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_dicts(image_folder):\n",
    "    \"\"\"\n",
    "    Returns Detectron2 style labels of images in image_folder based on data in annotations.\n",
    "    \n",
    "    TODO -- Maybe create some verbosity here? AKA, what are the outputs?\n",
    "    TODO -- what if annotations = None? Can we create a call to create an annotations CSV in 1 hit?\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    image_folder (str): target folder containing images\n",
    "    annotations (DataFrame): DataFrame of image label data\n",
    "    \"\"\"\n",
    "    annot_file = \"validation\" if \"valid\" in image_folder else \"train\"\n",
    "    print(f\"Using {annot_file} for annotations...\")\n",
    "    # Get annotations (automatically picks annotations CSV based on image_folder)\n",
    "    annotations = format_annotations(image_folder, \n",
    "                                     annot_file+\"-annotations-bbox.csv\",  # TODO: change for train/val\n",
    "                                     target_classes=target_classes) # TODO: this uses a global variable\n",
    "    \n",
    "    # TODO: Make sure to only use \"Coffeemaker\" class - FIX THIS Janky code!\n",
    "    #I COMMENTED OUT LINE BELOW BECAUSE I WANTED MORE THAN JUST COFFEEMAKERS\n",
    "    #annotations = annotations[annotations[\"ClassName\"] == \"Coffeemaker\"]\n",
    "\n",
    "    print(f\"On dataset: {annot_file}\")\n",
    "    print(\"Classes we're using: {}\".format(annotations[\"ClassName\"].value_counts()))\n",
    "\n",
    "    # Get all unique image ids from target folder\n",
    "    img_ids = get_image_ids(image_folder)\n",
    "    \n",
    "    # Start creating image dictionaries (Detectron2 style labelling)\n",
    "    img_dicts = []\n",
    "    for idx, img in enumerate(img_ids):\n",
    "        record = {}\n",
    "        \n",
    "        # Get image metadata\n",
    "        file_name = image_folder + \"/\" + img + \".jpg\"\n",
    "        height, width = cv2.imread(file_name).shape[:2]\n",
    "        img_data = annotations[annotations[\"ImageID\"] == img].reset_index() # reset index important for images\n",
    "                                                                            # with multiple objects\n",
    "        # Update record dictionary\n",
    "        record[\"file_name\"] = file_name\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        # Create list of image annotations (labels)\n",
    "        img_annotations = []\n",
    "        for i in range(len(img_data)): # this is where we loop through examples with multiple objects in an image\n",
    "            category_id = img_data.loc[i][\"ClassID\"].astype(\"object\") # JSON (for evalution) can't take int8 (NumPy type) must be native Python type\n",
    "            # Get bounding box coordinates in Detectron2 style (x0, y0, x1, y1)\n",
    "            bbox = np.float32(img_data.loc[i][[\"XMin\", \"YMin\", \"XMax\", \"YMax\"]].values) # needs to be float/int\n",
    "            # Convert bbox from relative to absolute pixel dimensions\n",
    "            bbox = rel_to_absolute(bbox=bbox, height=height, width=width)\n",
    "            # Setup annot (1 annot = 1 label, there might be more) dictionary\n",
    "            annot = {\n",
    "                \"bbox\": bbox,\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS, # See: https://detectron2.readthedocs.io/modules/structures.html#detectron2.structures.BoxMode.XYXY_ABS\n",
    "                \"category_id\": category_id\n",
    "            }\n",
    "            img_annotations.append(annot)\n",
    "            \n",
    "        # Update record dictionary with annotations\n",
    "        record[\"annotations\"] = img_annotations\n",
    "        \n",
    "        # Add record dictionary with image annotations to img_dicts list\n",
    "        img_dicts.append(record)\n",
    "    return img_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_img_dicts = get_image_dicts(train_path)\n",
    "val_img_dicts = get_image_dicts(valid_path)\n",
    "len(val_img_dicts), len(train_img_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_img_dicts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - Look at this yucky code... time to fix things up!\n",
    "unique_cats = []\n",
    "for dicty in train_img_dicts:\n",
    "  unique_cats.append(dicty[\"annotations\"][0][\"category_id\"])\n",
    "print(f\"Unique categories in train_img_dicts: {set(unique_cats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "for d in [valid_path, train_path]:\n",
    "    print(\"Registering: {}\".format(d.split(\"/\")[-1]))\n",
    "    DatasetCatalog.register(d.split(\"/\")[-1], lambda d=d: get_image_dicts(d))\n",
    "    MetadataCatalog.get(d.split(\"/\")[-1]).set(thing_classes=[\"Coffeemaker\",\"Dishwasher\",\"Sofa bed\"])\n",
    "coffee_metadata = MetadataCatalog.get(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_sample(data_dicts, metadata, n=1):\n",
    "  \"\"\"\n",
    "  Show n random samples from data_dicts, label with metadata.\n",
    "  \"\"\"\n",
    "  for d in random.sample(data_dicts, n):\n",
    "    print(d)\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1],\n",
    "                            metadata=metadata,\n",
    "                            scale=0.3)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "#     # Try plot with cv2 in Jupyter... (doesn't work)\n",
    "#     cv2.imshow('image', vis.get_image()[:, :, ::-1])\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyWindow('image')\n",
    "    # Try plot with matplotlib (works)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(vis.get_image()) #[:, :, ::-1])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualize_sample(val_img_dicts, \n",
    "                 coffee_metadata, \n",
    "                 n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation Begins\n",
    "1st is faster_rcnn_R_50 FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Has three classes\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"validation\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_img_dicts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "#dataset_dicts = get_balloon_dicts(\"balloon/val\")\n",
    "%matplotlib inline\n",
    "for d in random.sample(val_img_dicts, 1):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    print(outputs)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=coffee_metadata, \n",
    "                   scale=0.5 \n",
    "                   #instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(v.get_image()) #[:, :, ::-1])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    #cv2_imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"validation\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test two, RetinaNet 50 1x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR MODEL TWO - RETINANET 50 1x\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Has three classes\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"validation\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"validation\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retina Net R50 3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR MODEL TWO - RETINANET 50 1x\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Has three classes\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"validation\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"validation\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retina Net R 101 3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR MODEL TWO - RETINANET 50 1x\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Has three classes\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"validation\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"validation\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster rcnn R 50 3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR MODEL TWO - RETINANET 50 1x\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Has three classes\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"validation\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"validation\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster rcnn R 101 3x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Has three classes\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"validation\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"validation\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faster_rcnn_X_101 3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Has three classes\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"validation\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"validation\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
